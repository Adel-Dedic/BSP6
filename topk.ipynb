{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure that the tokenizer uses padding on the left side\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate words starting with specific letters\n",
    "def generate_with_start_letters(model, tokenizer, start_letters, max_length=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    generated = \"\"\n",
    "    input_ids = tokenizer.encode(tokenizer.bos_token, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    # Process each letter from the hidden message\n",
    "    for letter in start_letters:\n",
    "        found = False\n",
    "        attempts = 0\n",
    "        while not found and attempts < 10:\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length + len(generated.split()),\n",
    "                temperature=0.8,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "            if outputs.numel() == 0:\n",
    "                print(f\"No generation was produced for the letter '{letter}'.\")\n",
    "                generated += \" \" + letter.lower() + \"...\"\n",
    "                break\n",
    "\n",
    "            text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_words = text.split()[len(generated.split()):]\n",
    "            \n",
    "            for word in generated_words:\n",
    "                if word.lower().startswith(letter.lower()):\n",
    "                    generated += \" \" + word\n",
    "                    found = True\n",
    "                    input_ids = tokenizer.encode(generated, return_tensors='pt').to(device)\n",
    "                    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "                    break\n",
    "            attempts += 1\n",
    "\n",
    "    # After embedding the hidden message, generate naturally until a sentence terminator\n",
    "    sentence_completed = False\n",
    "    while not sentence_completed:\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length + len(generated.split()),\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        for i, char in enumerate(text[len(generated):]):\n",
    "            generated += char\n",
    "            if char in '.?!':  # Check for end of sentence punctuation\n",
    "                sentence_completed = True\n",
    "                break\n",
    "\n",
    "        # Update input_ids for continuation, if not finished\n",
    "        if not sentence_completed:\n",
    "            input_ids = tokenizer.encode(generated, return_tensors='pt').to(device)\n",
    "            attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    return generated.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I and found him to be a big part of my life.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_letters = ['I', 'A', 'F']\n",
    "sentence = generate_with_start_letters(model, tokenizer, start_letters)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Movies:   0%|          | 0/10000 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing Movies:   0%|          | 1/10000 [00:04<11:38:35,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing Movies:   0%|          | 2/10000 [00:12<18:02:31,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing Movies:   0%|          | 3/10000 [00:22<22:49:53,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing Movies:   0%|          | 4/10000 [00:28<20:23:05,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Movies:   0%|          | 4/10000 [00:28<20:06:57,  7.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     output_df\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mprocess_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArchive/TMDb_updated.CSV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1st_letter.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mprocess_movies\u001b[1;34m(csv_path, output_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     movie_title \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Adjust column name as per your CSV\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     start_letters \u001b[38;5;241m=\u001b[39m [char \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m movie_title \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39misalpha()]  \u001b[38;5;66;03m# Get letters from movie title\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_start_letters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_letters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Title\u001b[39m\u001b[38;5;124m'\u001b[39m: movie_title, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSteganographic Sentence\u001b[39m\u001b[38;5;124m'\u001b[39m: generated_text})\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Save results to a new CSV file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mgenerate_with_start_letters\u001b[1;34m(model, tokenizer, start_letters, max_length)\u001b[0m\n\u001b[0;32m     13\u001b[0m attempts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found \u001b[38;5;129;01mand\u001b[39;00m attempts \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo generation was produced for the letter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Wolverys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Wolverys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1575\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1558\u001b[0m     # 12. run assisted generate\n\u001b[0;32m   1559\u001b[0m     result = self._assisted_decoding(\n\u001b[0;32m   1560\u001b[0m         input_ids,\n\u001b[0;32m   1561\u001b[0m         candidate_generator=candidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1572\u001b[0m         **model_kwargs,\n\u001b[0;32m   1573\u001b[0m     )\n\u001b[0;32m   1574\u001b[0m if generation_mode == GenerationMode.GREEDY_SEARCH:\n\u001b[1;32m-> 1575\u001b[0m     # 11. run greedy search\n\u001b[0;32m   1576\u001b[0m     result = self._greedy_search(\n\u001b[0;32m   1577\u001b[0m         input_ids,\n\u001b[0;32m   1578\u001b[0m         logits_processor=prepared_logits_processor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1586\u001b[0m         **model_kwargs,\n\u001b[0;32m   1587\u001b[0m     )\n\u001b[0;32m   1589\u001b[0m elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n",
      "File \u001b[1;32mc:\\Users\\Wolverys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2759\u001b[0m, in \u001b[0;36m_sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2752\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2753\u001b[0m     output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[0;32m   2754\u001b[0m )\n\u001b[0;32m   2755\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2756\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   2757\u001b[0m )\n\u001b[0;32m   2758\u001b[0m return_dict_in_generate \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2759\u001b[0m     return_dict_in_generate\n\u001b[0;32m   2760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate\n\u001b[0;32m   2762\u001b[0m )\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;66;03m# init attention / hidden states / scores tuples\u001b[39;00m\n\u001b[0;32m   2765\u001b[0m scores \u001b[38;5;241m=\u001b[39m () \u001b[38;5;28;01mif\u001b[39;00m (return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wolverys\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\stopping_criteria.py:135\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEosTokenCriteria\u001b[39;00m(StoppingCriteria):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    This class can be used to stop generation whenever the \"end-of-sequence\" token is generated.\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;124;03m    By default, it uses the `model.generation_config.eos_token_id`.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m        eos_token_id (`Union[int, List[int]]`):\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m            The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, eos_token_id: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]):\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eos_token_id, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_movies(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    # Use tqdm for the progress bar, iterating over DataFrame rows\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Movies\"):\n",
    "        movie_title = row['title']  # Adjust column name as per your CSV\n",
    "        start_letters = [char for char in movie_title if char.isalpha()]  # Get letters from movie title\n",
    "        generated_text = generate_with_start_letters(model, tokenizer, start_letters)\n",
    "        results.append({'Original Title': movie_title, 'Steganographic Sentence': generated_text})\n",
    "\n",
    "    # Save results to a new CSV file\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "process_movies('Archive/TMDb_updated.CSV', '1st_letter.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
